{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66090e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df1 = pd.read_csv(\"bengaluru_house_prices.csv\")\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial data inspection\n",
    "print(\"Shape:\", df1.shape)\n",
    "print(\"Columns:\", df1.columns.tolist())\n",
    "summary = pd.DataFrame({\n",
    "    'dtype': df1.dtypes.astype(str),  \n",
    "    'missing': df1.isnull().sum(),\n",
    "    'unique': df1.nunique()\n",
    "}).sort_values(by='missing', ascending=False)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eba97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop columns with too many missing values or not useful\n",
    "df2 = df1.drop(['area_type','society','balcony','availability'], axis='columns')\n",
    "df2 = df2.dropna()\n",
    "df2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add BHK feature\n",
    "df2['bhk'] = df2['size'].apply(lambda x: int(x.split(' ')[0]))\n",
    "df2.bhk.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean total_sqft\n",
    "def convert_sqft_to_num(x):\n",
    "    try:\n",
    "        if '-' in x:\n",
    "            tokens = x.split('-')\n",
    "            return (float(tokens[0]) + float(tokens[1])) / 2\n",
    "        elif any(unit in x for unit in ['Sq. Meter', 'Sq. Yards', 'Acres', 'Guntha', 'Grounds', 'Cents', 'Perch']):\n",
    "            return None  # Simplified: drop rare units\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df2['total_sqft'] = df2['total_sqft'].apply(convert_sqft_to_num)\n",
    "df2 = df2.dropna(subset=['total_sqft'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create price_per_sqft\n",
    "df2['price_per_sqft'] = df2['price']*100000 / df2['total_sqft']\n",
    "df2 = df2[df2['price_per_sqft']<df2['price_per_sqft'].quantile(0.95)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8854f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce dimensionality of location\n",
    "df2['location'] = df2['location'].apply(lambda x: x.strip())\n",
    "location_stats = df2['location'].value_counts()\n",
    "location_less_than_10 = location_stats[location_stats <= 10]\n",
    "df2['location'] = df2['location'].apply(lambda x: 'other' if x in location_less_than_10 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final cleaned data\n",
    "df3 = df2.drop(['size', 'price_per_sqft'], axis='columns')\n",
    "dummies = pd.get_dummies(df3['location'])\n",
    "df4 = pd.concat([df3.drop('location', axis=1), dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40414aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features and Target\n",
    "X = df4.drop(['price'], axis=1)\n",
    "y = df4['price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc838a6",
   "metadata": {},
   "source": [
    "## Model Selection using GridSearchCV on Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def find_best_ensemble_model(X, y):\n",
    "    algos = {\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeRegressor(),\n",
    "            'params': {\n",
    "                'criterion': ['squared_error', 'friedman_mse'],\n",
    "                'splitter': ['best', 'random']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model': GradientBoostingRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'extra_trees': {\n",
    "            'model': ExtraTreesRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'max_depth': [None, 10]\n",
    "            }\n",
    "        },\n",
    "        'ada_boost': {\n",
    "            'model': AdaBoostRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'learning_rate': [0.5, 1.0]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': XGBRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "    for algo_name, config in algos.items():\n",
    "        gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n",
    "        gs.fit(X, y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "\n",
    "result_df = find_best_ensemble_model(X, y)\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
